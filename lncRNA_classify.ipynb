{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from multiprocess import Pool\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from os.path import abspath, join\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, plot_confusion_matrix, auc, plot_roc_curve\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are four training sets\n",
    "each of which are divided into three feature sets - fs1, fs2, fs3\n",
    "\n",
    "fs1 - sequence, GC content, kmer weights, with or without fickett score\n",
    "\n",
    "fs2 - fs1 + pairing probabilities\n",
    "\n",
    "fs3 - fs2 + conservation scores\n",
    "\n",
    "the pipeline is divided into three main parts:\n",
    "1) dataset curation with the basic features\n",
    "2) calculation of kmer weights and recording them in tuples (key, weight, frequency)\n",
    "3) training the random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File fs1/w_fickett/down_fickett does not exist: 'fs1/w_fickett/down_fickett'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f854de6132dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training_1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#the balanced dataset is loaded from each feature set-fickett score combination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mflanks_down\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fs1/w_fickett/down_fickett\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#w_fickett -> with fickett\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#wo_fickett -> without fickett\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/lncRNA_classify/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/lncRNA_classify/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/lncRNA_classify/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/lncRNA_classify/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/lncRNA_classify/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File fs1/w_fickett/down_fickett does not exist: 'fs1/w_fickett/down_fickett'"
     ]
    }
   ],
   "source": [
    "abspath(\"training_1\")\n",
    "#the balanced dataset is loaded from each feature set-fickett score combination\n",
    "flanks_down = pd.read_csv(\"fs1/w_fickett/down_fickett\")\n",
    "#w_fickett -> with fickett\n",
    "#wo_fickett -> without fickett\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flanks_down.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kmer weights are calculated and saved in tuples\n",
    "for every kmer-sequence combination a tuple is created where\n",
    "key: if the kmer is present in the sequence or not, [1,0]\n",
    "weight: the normalized kmer weight\n",
    "frequency: times the kmer occurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a multilabel binarizer is employed to create a feature from each of the tuple elements\n",
    "cmlb=pd.read_hdf(\"fs1/cmlb5\", key=\"mlb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the final training set is created combining all the features\n",
    "features_combined = pd.concat([flanks_down, cmlb], axis=1, copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we split the training set 80-20 for the classifier to train on\n",
    "\n",
    "features_combined.drop(['gene_id'], 1, inplace=True)\n",
    "\n",
    "#for phatscons/fs3\n",
    "#val = {'cons_1':0, 'cons_2':0, 'cons_3':0, 'cons_4':0, 'cons_5':0, 'cons_6':0, \n",
    "#'cons_7':0, 'cons_8':0, 'cons_9':0, 'cons_10':0, 'cons_11':0, 'cons_12':0, 'cons_13':0, \n",
    "#'cons_14':0, 'cons_15':0, 'cons_16':0, 'cons_17':0, 'cons_18':0, 'cons_19':0, 'cons_20':0}\n",
    "#features_combined_3.fillna(value=val, inplace=True)\n",
    "\n",
    "\n",
    "ncrna_train, ncrna_test = train_test_split(features_combined_3, test_size=0.2, random_state=42)\n",
    "\n",
    "ncrna_test_target=ncrna_test[[\"target\"]]\n",
    "ncrna_test=ncrna_test.drop([\"target\"],1)\n",
    "print (ncrna_test_target.shape)\n",
    "print (ncrna_test.shape)\n",
    "\n",
    "ncrna_train_target=ncrna_train[[\"target\"]]\n",
    "ncrna_train=ncrna_train.drop([\"target\"],1)\n",
    "print (ncrna_train_target.shape)\n",
    "print (ncrna_train.shape)\n",
    "\n",
    "#separating numerical features and categorical features\n",
    "num_feat=[]\n",
    "for x in ncrna_train.columns[1:]:\n",
    "    num_feat.append(x)\n",
    "\n",
    "cat_feat = [\"seq\"]\n",
    "\n",
    "#encoding the sequence feature\n",
    "#scaling the rest of the features\n",
    "#and creating a ColumnTransformer object\n",
    "cct = ColumnTransformer([\n",
    "    ('oe', OrdinalEncoder(), cat_feat),\n",
    "    ('num', StandardScaler(), num_feat)\n",
    "    ], remainder='passthrough')\n",
    "\n",
    "#fitting the split training set and test set\n",
    "X_train = cct.fit_transform(ncrna_train)\n",
    "X_test = cct.fit_transform(ncrna_test)\n",
    "\n",
    "#encoding the target labels\n",
    "ordinal = OrdinalEncoder()\n",
    "y_train = ordinal.fit_transform(ncrna_train_target)\n",
    "y_test = ordinal.fit_transform(ncrna_test_target)\n",
    "\n",
    "y_train = np.ravel(y_train)\n",
    "\n",
    "print (y_train.shape)\n",
    "print (y_test.shape)\n",
    "\n",
    "print (X_train.shape)\n",
    "print (X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the random forest classifier with default params\n",
    "rnd_clf = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=42)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "y_pred = rnd_clf.predict(X_test)\n",
    "print (f'accuracy: {accuracy_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid search with 10-fold cv\n",
    "param_grid = {'oob_score': [True, False], 'bootstrap':[True],\n",
    "              'criterion': [\"gini\", \"entropy\"], 'n_estimators':[100,300,500,1000]}\n",
    "grid_search_def10 = GridSearchCV(RandomForestClassifier(random_state=42),\n",
    "    param_grid, scoring=\"accuracy\", n_jobs=-1, verbose=1, cv=10)\n",
    "grid_search_def10.fit(X_train, y_train)\n",
    "\n",
    "print (f\"Grid search cv10 best score: {grid_search_def10.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid search with 5-fold cv starts\n",
    "param_grid = {'oob_score': [True, False], 'bootstrap': [True],\n",
    "              'criterion': [\"gini\", \"entropy\"], 'n_estimators':[100,300,500,1000]}\n",
    "grid_search_def5 = GridSearchCV(RandomForestClassifier(random_state=42),\n",
    "    param_grid, scoring=\"accuracy\", n_jobs=-1, verbose=1, cv=5)\n",
    "grid_search_def5.fit(X_train, y_train)\n",
    "\n",
    "print (f\"Grid search cv5 best score: {grid_search_def5.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model 1 is the best estimator from the grid search with 10-fold cv\n",
    "model_1 = grid_search_def10.best_estimator_\n",
    "y_pred = model_1.predict(X_test)\n",
    "model_1_acc = accuracy_score(y_test, y_pred)\n",
    "print (f\"model 1 accuracy: {model_1_acc}\")\n",
    "y_pred_train = model_1.predict(X_train)\n",
    "model_1_train_acc = accuracy_score(y_train, y_pred_train)\n",
    "print (f\"model 1 accuracy on training set: {model_1_train_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model 2 is the best estimator from the grid search with 5-fold cv\n",
    "model_2 = grid_search_def5.best_estimator_\n",
    "y_pred_5 = model_2.predict(X_test)\n",
    "model_2_acc = accuracy_score(y_test, y_pred_5)\n",
    "print (f\"model 2 accuracy: {model_2_acc}\")\n",
    "y_pred_train_5 = model_2.predict(X_train)\n",
    "model_2_train_acc = accuracy_score(y_train, y_pred_train_5)\n",
    "print (f\"model 2 accuracy on training set: {model_2_train_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original model \n",
    "y_pred_orig = rnd_clf.predict(X_test)\n",
    "print (accuracy_score(y_test, y_pred_orig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification matrix for the original model\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "path = \"fs1/w_fickett/results\"\n",
    "if not os.path.isdir(join(path)):\n",
    "    os.makedirs(join(path))\n",
    "plt.savefig(join(path,\"feat_imp.png\"))\n",
    "\n",
    "disp = plot_confusion_matrix(rnd_clf, X_test, y_test,\n",
    "                                     display_labels=[\"lnc\",\"mir\",\"sno\"],\n",
    "                                     cmap=plt.cm.Blues)\n",
    "disp.ax_.set_title('feature set #1 with fickett score')\n",
    "print (disp.confusion_matrix)\n",
    "plt.savefig(join(path,\"conf_mat.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature importances\n",
    "feat_imp = rnd_clf.feature_importances_\n",
    "importances = pd.Series(feat_imp, index=features_combined.drop([\"target\"],1).columns)\n",
    "importances.nlargest(10).plot(kind=\"barh\",figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the next section follows the unsupervised learning setup.\n",
    "k-means clustering of the three RNA classes and PCA with two components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-means clustering of the three classes\n",
    "km = KMeans (n_clusters=3, init=\"k-means++\", random_state=42).fit(X_train)\n",
    "\n",
    "y = km.predict(X_test)\n",
    "\n",
    "print (f'number of iterations : {km.n_iter_}\\n')\n",
    "\n",
    "#labels : {0: lnc, 1: mir, 2:sno}\n",
    "#training set\n",
    "print(f'predicted classes on the training set: {Counter(km.labels_)}\\n')\n",
    "print(f'actual classes on the trainig set: {Counter(y_train)}\\n')\n",
    "print(f'accuracy training set: {accuracy_score(y_train, km.labels_)}\\n')\n",
    "print(f'number of correctly classified samples: {accuracy_score(y_train, km.labels_, normalize=False)}\\n')\n",
    "\n",
    "#test set\n",
    "print(f'predicted classes on the test set: {Counter(y)}\\n')\n",
    "print(f'actual classes on the test set: {Counter(y_test)}\\n') \n",
    "print(f'accuracy test set: {accuracy_score(y_test, y)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pca\n",
    "p = PCA(n_components=2, random_state=42)\n",
    "\n",
    "X= p.fit_transform(X_train)\n",
    "\n",
    "print(f'explained variance ratio: {p.explained_variance_ratio_}')\n",
    "print(f'n_compponents: {p.n_components_}')\n",
    "print(f'features: {p.n_features_}')\n",
    "print(f'samples: {p.n_samples_}')\n",
    "\n",
    "fig = plt.figure(figsize=(8,3))\n",
    "colours = ['navy', 'turquoise', 'darkorange']\n",
    "targets = ['lnc', 'mir', 'sno']\n",
    "lw=2\n",
    "for colour, i, target_name in zip(colours, [0, 1, 2], targets):\n",
    "    plt.scatter(X[y_train == i, 0], X[y_train == i, 1], color=colour, alpha=.8, #lw=lw,\n",
    "                label=target_name)\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
