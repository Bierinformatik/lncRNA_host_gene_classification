{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from multiprocess import Pool\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from os.path import abspath, join\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, plot_confusion_matrix, auc, plot_roc_curve\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are four training sets\n",
    "each of which are divided into three feature sets - fs1, fs2, fs3\n",
    "\n",
    "fs1 - sequence, GC content, kmer weights, with or without fickett score\n",
    "fs2 - fs1 + pairing probabilities\n",
    "fs3 - fs2 + conservation scores\n",
    "\n",
    "the pipeline is divided into three main parts:\n",
    "1) dataset curation with the basic features\n",
    "2) calculation of kmer weights and recording them in tuples (key, weight, frequency)\n",
    "3) training the random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abspath(\"training_1\")\n",
    "#the balanced dataset is loaded from each feature set-fickett score combination\n",
    "flanks_down = pd.read_csv(\"fs1/w_fickett/down_fickett\")\n",
    "#w_fickett -> with fickett\n",
    "#wo_fickett -> without fickett\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flanks_down.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kmer weights are calculated and saved in tuples\n",
    "for every kmer-sequence combination a tuple is created where\n",
    "key: if the kmer is present in the sequence or not, [1,0]\n",
    "weight: the normalized kmer weight\n",
    "frequency: times the kmer occurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a multilabel binarizer is employed to create a feature from each of the tuple elements\n",
    "cmlb=pd.read_hdf(\"fs1/cmlb5\", key=\"mlb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the final training set is created combining all the features\n",
    "features_combined = pd.concat([flanks_down, cmlb], axis=1, copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we split the training set 80-20 for the classifier to train on\n",
    "\n",
    "features_combined.drop(['gene_id'], 1, inplace=True)\n",
    "\n",
    "#for phatscons/fs3\n",
    "#val = {'cons_1':0, 'cons_2':0, 'cons_3':0, 'cons_4':0, 'cons_5':0, 'cons_6':0, \n",
    "#'cons_7':0, 'cons_8':0, 'cons_9':0, 'cons_10':0, 'cons_11':0, 'cons_12':0, 'cons_13':0, \n",
    "#'cons_14':0, 'cons_15':0, 'cons_16':0, 'cons_17':0, 'cons_18':0, 'cons_19':0, 'cons_20':0}\n",
    "#features_combined_3.fillna(value=val, inplace=True)\n",
    "\n",
    "\n",
    "ncrna_train, ncrna_test = train_test_split(features_combined_3, test_size=0.2, random_state=42)\n",
    "\n",
    "ncrna_test_target=ncrna_test[[\"target\"]]\n",
    "ncrna_test=ncrna_test.drop([\"target\"],1)\n",
    "print (ncrna_test_target.shape)\n",
    "print (ncrna_test.shape)\n",
    "\n",
    "ncrna_train_target=ncrna_train[[\"target\"]]\n",
    "ncrna_train=ncrna_train.drop([\"target\"],1)\n",
    "print (ncrna_train_target.shape)\n",
    "print (ncrna_train.shape)\n",
    "\n",
    "#separating numerical features and categorical features\n",
    "num_feat=[]\n",
    "for x in ncrna_train.columns[1:]:\n",
    "    num_feat.append(x)\n",
    "\n",
    "cat_feat = [\"seq\"]\n",
    "\n",
    "#encoding the sequence feature\n",
    "#scaling the rest of the features\n",
    "#and creating a ColumnTransformer object\n",
    "cct = ColumnTransformer([\n",
    "    ('oe', OrdinalEncoder(), cat_feat),\n",
    "    ('num', StandardScaler(), num_feat)\n",
    "    ], remainder='passthrough')\n",
    "\n",
    "#fitting the split training set and test set\n",
    "X_train = cct.fit_transform(ncrna_train)\n",
    "X_test = cct.fit_transform(ncrna_test)\n",
    "\n",
    "#encoding the target labels\n",
    "ordinal = OrdinalEncoder()\n",
    "y_train = ordinal.fit_transform(ncrna_train_target)\n",
    "y_test = ordinal.fit_transform(ncrna_test_target)\n",
    "\n",
    "y_train = np.ravel(y_train)\n",
    "\n",
    "print (y_train.shape)\n",
    "print (y_test.shape)\n",
    "\n",
    "print (X_train.shape)\n",
    "print (X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the random forest classifier with default params\n",
    "rnd_clf = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=42)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "y_pred = rnd_clf.predict(X_test)\n",
    "print (f'accuracy: {accuracy_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid search with 10-fold cv\n",
    "param_grid = {'oob_score': [True, False], 'bootstrap':[True],\n",
    "              'criterion': [\"gini\", \"entropy\"], 'n_estimators':[100,300,500,1000]}\n",
    "grid_search_def10 = GridSearchCV(RandomForestClassifier(random_state=42),\n",
    "    param_grid, scoring=\"accuracy\", n_jobs=-1, verbose=1, cv=10)\n",
    "grid_search_def10.fit(X_train, y_train)\n",
    "\n",
    "print (f\"Grid search cv10 best score: {grid_search_def10.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid search with 5-fold cv starts\n",
    "param_grid = {'oob_score': [True, False], 'bootstrap': [True],\n",
    "              'criterion': [\"gini\", \"entropy\"], 'n_estimators':[100,300,500,1000]}\n",
    "grid_search_def5 = GridSearchCV(RandomForestClassifier(random_state=42),\n",
    "    param_grid, scoring=\"accuracy\", n_jobs=-1, verbose=1, cv=5)\n",
    "grid_search_def5.fit(X_train, y_train)\n",
    "\n",
    "print (f\"Grid search cv5 best score: {grid_search_def5.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model 1 is the best estimator from the grid search with 10-fold cv\n",
    "model_1 = grid_search_def10.best_estimator_\n",
    "y_pred = model_1.predict(X_test)\n",
    "model_1_acc = accuracy_score(y_test, y_pred)\n",
    "print (f\"model 1 accuracy: {model_1_acc}\")\n",
    "y_pred_train = model_1.predict(X_train)\n",
    "model_1_train_acc = accuracy_score(y_train, y_pred_train)\n",
    "print (f\"model 1 accuracy on training set: {model_1_train_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model 2 is the best estimator from the grid search with 5-fold cv\n",
    "model_2 = grid_search_def5.best_estimator_\n",
    "y_pred_5 = model_2.predict(X_test)\n",
    "model_2_acc = accuracy_score(y_test, y_pred_5)\n",
    "print (f\"model 2 accuracy: {model_2_acc}\")\n",
    "y_pred_train_5 = model_2.predict(X_train)\n",
    "model_2_train_acc = accuracy_score(y_train, y_pred_train_5)\n",
    "print (f\"model 2 accuracy on training set: {model_2_train_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original model \n",
    "y_pred_orig = rnd_clf.predict(X_test)\n",
    "print (accuracy_score(y_test, y_pred_orig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification matrix for the original model\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "path = \"fs1/w_fickett/results\"\n",
    "if not os.path.isdir(join(path)):\n",
    "    os.makedirs(join(path))\n",
    "plt.savefig(join(path,\"feat_imp.png\"))\n",
    "\n",
    "disp = plot_confusion_matrix(rnd_clf, X_test, y_test,\n",
    "                                     display_labels=[\"lnc\",\"mir\",\"sno\"],\n",
    "                                     cmap=plt.cm.Blues)\n",
    "disp.ax_.set_title('feature set #1 with fickett score')\n",
    "print (disp.confusion_matrix)\n",
    "plt.savefig(join(path,\"conf_mat.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature importances\n",
    "feat_imp = rnd_clf.feature_importances_\n",
    "importances = pd.Series(feat_imp, index=features_combined.drop([\"target\"],1).columns)\n",
    "importances.nlargest(10).plot(kind=\"barh\",figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the next section follows the unsupervised learning setup.\n",
    "k-means clustering of the three RNA classes and PCA with two components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-means clustering of the three classes\n",
    "km = KMeans (n_clusters=3, init=\"k-means++\", random_state=42).fit(X_train)\n",
    "\n",
    "y = km.predict(X_test)\n",
    "\n",
    "print (f'number of iterations : {km.n_iter_}\\n')\n",
    "\n",
    "#labels : {0: lnc, 1: mir, 2:sno}\n",
    "#training set\n",
    "print(f'predicted classes on the training set: {Counter(km.labels_)}\\n')\n",
    "print(f'actual classes on the trainig set: {Counter(y_train)}\\n')\n",
    "print(f'accuracy training set: {accuracy_score(y_train, km.labels_)}\\n')\n",
    "print(f'number of correctly classified samples: {accuracy_score(y_train, km.labels_, normalize=False)}\\n')\n",
    "\n",
    "#test set\n",
    "print(f'predicted classes on the test set: {Counter(y)}\\n')\n",
    "print(f'actual classes on the test set: {Counter(y_test)}\\n') \n",
    "print(f'accuracy test set: {accuracy_score(y_test, y)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pca\n",
    "p = PCA(n_components=2, random_state=42)\n",
    "\n",
    "X= p.fit_transform(X_train)\n",
    "\n",
    "print(f'explained variance ratio: {p.explained_variance_ratio_}')\n",
    "print(f'n_compponents: {p.n_components_}')\n",
    "print(f'features: {p.n_features_}')\n",
    "print(f'samples: {p.n_samples_}')\n",
    "\n",
    "fig = plt.figure(figsize=(8,3))\n",
    "colours = ['navy', 'turquoise', 'darkorange']\n",
    "targets = ['lnc', 'mir', 'sno']\n",
    "lw=2\n",
    "for colour, i, target_name in zip(colours, [0, 1, 2], targets):\n",
    "    plt.scatter(X[y_train == i, 0], X[y_train == i, 1], color=colour, alpha=.8, #lw=lw,\n",
    "                label=target_name)\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
